{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f5f8f0",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a",
    "execution": {
     "iopub.execute_input": "2024-03-25T11:25:27.736613Z",
     "iopub.status.busy": "2024-03-25T11:25:27.735774Z",
     "iopub.status.idle": "2024-03-25T11:26:24.271917Z",
     "shell.execute_reply": "2024-03-25T11:26:24.270721Z"
    },
    "papermill": {
     "duration": 56.551022,
     "end_time": "2024-03-25T11:26:24.278978",
     "exception": false,
     "start_time": "2024-03-25T11:25:27.727956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==3.3.4 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ceb1a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T11:26:24.291866Z",
     "iopub.status.busy": "2024-03-25T11:26:24.291407Z",
     "iopub.status.idle": "2024-03-25T11:26:24.296838Z",
     "shell.execute_reply": "2024-03-25T11:26:24.295668Z"
    },
    "papermill": {
     "duration": 0.015289,
     "end_time": "2024-03-25T11:26:24.299477",
     "exception": false,
     "start_time": "2024-03-25T11:26:24.284188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e20719",
   "metadata": {
    "papermill": {
     "duration": 0.004512,
     "end_time": "2024-03-25T11:26:24.308833",
     "exception": false,
     "start_time": "2024-03-25T11:26:24.304321",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7eb008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T11:26:24.321472Z",
     "iopub.status.busy": "2024-03-25T11:26:24.320556Z",
     "iopub.status.idle": "2024-03-25T11:56:34.880717Z",
     "shell.execute_reply": "2024-03-25T11:56:34.879259Z"
    },
    "papermill": {
     "duration": 1810.570186,
     "end_time": "2024-03-25T11:56:34.883779",
     "exception": false,
     "start_time": "2024-03-25T11:26:24.313593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/25 11:26:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "q5 start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5: <72,861,932,6086>\n",
      "Q5: <72,861,1793,6086>\n",
      "Q5: <72,932,1793,5663>\n",
      "Q5: <112,563,861,7029>\n",
      "Q5: <112,773,861,6973>\n",
      "Q5: <112,845,861,6959>\n",
      "Q5: <227,861,1087,6158>\n",
      "Q5: <227,861,1948,6158>\n",
      "Q5: <227,1087,1948,5725>\n",
      "Q5: <373,861,1233,6204>\n",
      "Q5: <373,861,2094,6204>\n",
      "Q5: <373,1233,2094,5772>\n",
      "Q5: <563,773,861,6957>\n",
      "Q5: <563,845,861,6939>\n",
      "Q5: <773,845,861,6887>\n",
      "Q5: <861,932,1793,6086>\n",
      "Q5: <861,1087,1948,6158>\n",
      "Q5: <861,1233,2094,6204>\n",
      "2.9101721339999997\n",
      "Completed run !\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext, RDD\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "\n",
    "# The code below may help you if your pc cannot find the correct python executable.\n",
    "# Don't use this code on the server!\n",
    "# import os\n",
    "# import sys\n",
    "# os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# TODO: Make sure that if you installed Spark version 3.3.4 (recommended) that you install the same version of\n",
    "#  PySpark. You can do this by running the following command: pip install pyspark==3.3.4\n",
    "\n",
    "def get_spark_context(on_server: bool) -> SparkContext:\n",
    "    spark_conf = SparkConf().setAppName(\"2AMD15\")\n",
    "    if not on_server:\n",
    "        spark_conf = spark_conf.setMaster(\"local[*]\") # check this\n",
    "\n",
    "    spark_context = SparkContext.getOrCreate(spark_conf)\n",
    "\n",
    "    if on_server:\n",
    "        # TODO: You may want to change ERROR to WARN to receive more info. For larger data sets, to not set the\n",
    "        #  log level to anything below WARN, Spark will print too much information.\n",
    "        spark_context.setLogLevel(\"WARN\")\n",
    "\n",
    "    return spark_context\n",
    "\n",
    "\n",
    "def q0a(spark_context: SparkContext, on_server: bool) -> DataFrame:\n",
    "    plays_file_path = \"/plays.txt\" if on_server else \"/kaggle/input/bigdm/plays.txt\"\n",
    "\n",
    "    spark_session = SparkSession(spark_context)\n",
    "\n",
    "    # Write code in Spark to import the dataset: (a) as a data frame/dataset, and (b) as an RDD. Even\n",
    "    # though this question receives 0 points, you will rely on this code for the following questions.\n",
    "    # Therefore, if your code for this question is wrong, this will most likely lead to wrong answers for\n",
    "    # the following questions.\n",
    "    \n",
    "    # TODO: Implement Q0a here by creating a Dataset of DataFrame out of the file at {@code plays_file_path}.\n",
    "    df = spark_session.read.format(\"text\").load(plays_file_path)\n",
    "\n",
    "    # Split the values column based on the comma delimiter\n",
    "    split_col = split(df['value'], ',')\n",
    "\n",
    "    # Add the split columns to the DataFrame\n",
    "    df = df.withColumn('userid', split_col.getItem(0))\n",
    "    df = df.withColumn('songid', split_col.getItem(1))\n",
    "    df = df.withColumn('rating', split_col.getItem(2))\n",
    "\n",
    "    # Drop the original values column\n",
    "    df = df.drop('value')\n",
    "    print('q0a:')\n",
    "    print(df.show(1))\n",
    "    print('q0a end')\n",
    "    df.createOrReplaceTempView(\"assignment\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def q0b(spark_context: SparkContext, on_server: bool) -> RDD:\n",
    "    plays_file_path = \"/plays.txt\" if on_server else \"/kaggle/input/bigdm/plays.txt\"\n",
    "\n",
    "    # TODO: Implement Q0b here by creating an RDD out of the file at {@code plays_file_path}.\n",
    "\n",
    "    rdd_df = spark_context.textFile(plays_file_path)\n",
    "    rdd_df_2 = rdd_df.map(lambda x: x.split(\",\"))    \n",
    "\n",
    "    return rdd_df_2\n",
    "\n",
    "\n",
    "def q1(spark_context: SparkContext, data_frame: DataFrame):\n",
    "    \n",
    "    \n",
    "    spark_session = SparkSession(spark_context)\n",
    "\n",
    "\n",
    "    start = time.process_time()\n",
    "    \n",
    "    print('q1:')\n",
    "    spark_session.sql(\"\"\"SELECT COUNT(*) FROM (SELECT userid FROM assignment\n",
    "    WHERE rating is not NULL\n",
    "    GROUP BY userid  HAVING COUNT(rating) >= 100 AND \n",
    "    AVG(rating) < 2 )\"\"\").show(50)\n",
    "    \n",
    "    print(time.process_time() - start)\n",
    "    print('q1 end')\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def q2(spark_context: SparkContext, rdd: RDD):\n",
    "    \n",
    "    #The user provided at least 100 ratings, and,\n",
    "    #The user has the minimum average rating, compared to all \n",
    "    #users that provided at least 100 ratings\n",
    "    print(rdd.map(lambda s: len(s)))\n",
    "\n",
    "    \n",
    "    return\n",
    "\n",
    "def q3(spark_context: SparkContext, rdd: RDD):\n",
    "    # TODO: Implement Q3 here\n",
    "    return\n",
    "\n",
    "\n",
    "def q4(spark_context: SparkContext, rdd: RDD):\n",
    "    # TODO: Implement Q4 here\n",
    "    return\n",
    "\n",
    "def q5(spark_context: SparkContext, rdd: RDD):\n",
    "    # 6 min with 4 partitions\n",
    "    # filter before sort?\n",
    "    # try to reduce caching?\n",
    "    # remove users below certain threshold after first cartesian?\n",
    "    # remove broadcast?\n",
    "    \n",
    "    print(\"q5 start\")\n",
    "\n",
    "    rdd = rdd.repartition(4)\n",
    "    start = time.process_time()\n",
    "\n",
    "    filtered_rdd = rdd.filter(lambda x: len(x) > 2)\n",
    "    userSongPairs = filtered_rdd.map(lambda x: (int(x[0]), int(x[1])))\n",
    "\n",
    "    userSongCounts = userSongPairs.aggregateByKey(\n",
    "        set(),\n",
    "        lambda songs, song: songs | {song},\n",
    "        lambda songs1, songs2: songs1 | songs2\n",
    "    ).filter(lambda x: len(x[1]) <= 8000)\n",
    "    \n",
    "    userSongsBroadcast: Broadcast[dict] = spark_context.broadcast(userSongCounts.collectAsMap())\n",
    "\n",
    "    users = userSongCounts.map(lambda x: x[0]) # cache?\n",
    "\n",
    "    def count_distinct_songs(*userslist):\n",
    "        user_songs = userSongsBroadcast.value\n",
    "        distinct_songs = set()\n",
    "        for user in userslist:\n",
    "            user_songs_set = user_songs.get(user, set())\n",
    "            distinct_songs |= user_songs_set\n",
    "        return len(distinct_songs)\n",
    "\n",
    "    user_combinations = users.cartesian(users)\n",
    "    \n",
    "    users_filtered = user_combinations.filter(lambda x: x[0] < x[1])\n",
    "\n",
    "    distinctSongs = users_filtered.map(lambda x: ((x[0], x[1]), count_distinct_songs(x[0], x[1])))\n",
    "\n",
    "    filtering1 = distinctSongs.filter(lambda x: x[1] <= 8000)\n",
    "\n",
    "    justUsers = filtering1.map(lambda x: (x[0][0], x[0][1]))\n",
    "    #.sortBy(lambda x: (x[0], x[1]))\n",
    "        \n",
    "    cartesian2 = justUsers.cartesian(users)\n",
    "    \n",
    "    cartesian2_filtered = cartesian2.filter(lambda x: x[0][1] < x[1])\n",
    "    \n",
    "    distinctSongs2 = cartesian2_filtered.map(lambda x: (x[0][0],x[0][1],x[1],count_distinct_songs(x[0][0],x[0][1],x[1])))\n",
    "\n",
    "    filtering2 = distinctSongs2.filter(lambda x: x[3] <= 8000)\n",
    "    \n",
    "    finalsorted = filtering2.sortBy(lambda x: (x[0], x[1], x[2]))\n",
    "    \n",
    "    for user in finalsorted.collect():\n",
    "        print(\"Q5:\", \"<\" + \",\".join(map(str, user)) + \">\")\n",
    "\n",
    "    print(time.process_time() - start)\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    print(\"Preparing to run...\")\n",
    "    on_server = False  # TODO: Set this to true if and only if deploying to the server\n",
    "\n",
    "    spark_context = get_spark_context(on_server)\n",
    "\n",
    "    #data_frame = q0a(spark_context, on_server)\n",
    "\n",
    "    rdd = q0b(spark_context, on_server)\n",
    "\n",
    "    #q1(spark_context, data_frame)\n",
    "\n",
    "    #q2(spark_context, rdd)\n",
    "\n",
    "    #q4(spark_context, rdd)\n",
    "\n",
    "    q5(spark_context, rdd)\n",
    "\n",
    "    spark_context.stop()\n",
    "    print(\"Completed run !\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f09530",
   "metadata": {
    "papermill": {
     "duration": 0.025783,
     "end_time": "2024-03-25T11:56:34.935560",
     "exception": false,
     "start_time": "2024-03-25T11:56:34.909777",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "580c9f17",
   "metadata": {
    "papermill": {
     "duration": 0.025561,
     "end_time": "2024-03-25T11:56:34.986648",
     "exception": false,
     "start_time": "2024-03-25T11:56:34.961087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "b=[1,2,3]\n",
    "\n",
    "add_one = lambda x: x[1] + 10\n",
    "add_one(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704ec13",
   "metadata": {
    "papermill": {
     "duration": 0.025427,
     "end_time": "2024-03-25T11:56:35.036923",
     "exception": false,
     "start_time": "2024-03-25T11:56:35.011496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUESTION 1\n",
    "\n",
    "   The average rating of a user is the mean value of \n",
    "    all ratings provided by that user. For example,\n",
    "    assuming that we have the following information for user 1:\n",
    "    1, 44, 3\n",
    "    1, 45, 5\n",
    "    1, 56\n",
    "    1, 53\n",
    "    1, 46, 1\n",
    "    The average rating for the user is (3+5+1)/3. The definition of average rating does not account\n",
    "    for the lines without a rating.\n",
    "    Write code that uses SparkSQL to find the total number of \n",
    "    all users that provided at least 100\n",
    "    ratings, and have an average rating below 2.\n",
    "    Your answer should be printed as follows:\n",
    "    >> Q1: number\n",
    "    \n",
    "\n",
    "Constraints:\n",
    "a) Your code should achieve the maximum degree of \n",
    "parallelism on the available hardware.\n",
    "The code that is executed centrally (on the master node) should be of constant\n",
    "complexity. This also means that the code executed on the \n",
    "master should contain no loops.\n",
    "\n",
    "b) The actual computation of your answer should be with a single standard SQL\n",
    "command, i.e., SQL that could also run in a relational database. \n",
    "Besides creating a view or a temporary table (which may use the dataframe API) \n",
    "you should not use the dataframe API for the actual computation of the answer.\n",
    "For example, the following command:\n",
    "\n",
    "spark.sql(“SELECT name FROM people WHERE age>10”).show()\n",
    "\n",
    "satisfies this constraint (albeit, it does not correctly answer the question), \n",
    "\n",
    "whereas df.filter(col(\"age\").gt(10)).select(“name”);\n",
    "\n",
    "does not satisfy this constraint because it uses the dataframe API on \n",
    "something else than creating a view or a temporary table \n",
    "(in this example, for filtering and for projection).\n",
    "Report/deliver the following information/code:\n",
    "\n",
    "a) Fill in the missing code in question1.java or question1.py for \n",
    "answering the question.\n",
    "b) The SQL query [Report and poster]\n",
    "c) The output of the query plan [Report]\n",
    "d) A paragraph (or pseudocode) describing how you would implement the same\n",
    "functionality in Spark, but without using SparkSQL. \n",
    "\n",
    "Notice that you do not need to actually implement it. \n",
    "[Report and short discussion in the poster]\n",
    "e) A paragraph where you compare your Spark implementation (point d above) \n",
    "and the SparkSQL implementation, in terms of efficiency. \n",
    "The paragraph should solely rely on the query plan of SparkSQL, \n",
    "and on your answer on point d above, i.e., without actually implementing \n",
    "point d [Report and short discussion in poster]\n",
    "\n",
    "The expected length of this question in the report is 1 page. Sub-questions \n",
    "a, b, c receive 10\n",
    "points total, and points d and e receive 10 points total (assuming that the previous\n",
    "sub-questions are correct).\n",
    "Hint: For all questions, you can test that your SQL \n",
    "    answers are correct on the small dataset, for\n",
    "which we provide the expected answers. \n",
    "Keep in mind that correct answers on the small\n",
    "dataset do not guarantee that your code is correct. \n",
    "However, wrong answers on the small\n",
    "dataset mean that your code is wrong.\n",
    "Answer on the small dataset:\n",
    ">> Q1: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a3bb9",
   "metadata": {
    "papermill": {
     "duration": 0.024907,
     "end_time": "2024-03-25T11:56:35.087112",
     "exception": false,
     "start_time": "2024-03-25T11:56:35.062205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUESTION 2\n",
    "\n",
    "You are asked to write a Spark program (not using SparkSQL) for finding the grumpiest user.\n",
    "The formal definition of the grumpiest user is as follows:\n",
    "\n",
    "a) The user provided at least 100 ratings, and,\n",
    "b) The user has the minimum average rating, compared to all users that provided at least\n",
    "100 ratings.\n",
    "\n",
    "You can assume that there is exactly one grumpiest user, i.e., no two users have an identical\n",
    "minimum average rating!\n",
    "\n",
    "Notice that the definition of the grumpiest user only considers the users with at least 100 ratings!\n",
    "\n",
    "Failing to take this into account will lead to wrong results.\n",
    "Your answer should be printed as follows:\n",
    ">> Q2: userid, averagerating\n",
    "For example,\n",
    ">> Q2: 861, 1.0\n",
    "\n",
    "Constraints:\n",
    "a) Your code should have a high (but configurable) degree of parallelism. The code that is\n",
    "executed centrally (on the master node) should be of constant complexity. This also\n",
    "means that the code executed on the master should contain no loops.\n",
    "b) Your answer should not use SQL or the dataframe API. This also means that you should\n",
    "not use the groupBy function. Think of how this functionality could be done with\n",
    "reduceByKey instead!\n",
    "Report/deliver the following information/code:\n",
    "\n",
    "a) Fill in the missing method in question2.java or question2.py.\n",
    "b) Brief description of your solution [Report and poster]\n",
    "c) Investigate how parallelism helps on the performance of your code in Question 2. In\n",
    "particular, test how fast is your code for different degrees of parallelism (indicatively, 1, 5,\n",
    "10, 20, 40, 80). Plot, discuss, and explain your results. [Report and poster]\n",
    "The expected length of this question in the report is 1 page. Points a and b receive 10 points\n",
    "total, and point c receives 10 points total (assuming that the previous points are correct).\n",
    "Answer on the small dataset:\n",
    ">> Q2: 861, 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f0abc6",
   "metadata": {
    "papermill": {
     "duration": 0.024995,
     "end_time": "2024-03-25T11:56:35.137005",
     "exception": false,
     "start_time": "2024-03-25T11:56:35.112010",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76bbd190",
   "metadata": {
    "papermill": {
     "duration": 0.027156,
     "end_time": "2024-03-25T11:56:35.189990",
     "exception": false,
     "start_time": "2024-03-25T11:56:35.162834",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91c2a6a4",
   "metadata": {
    "papermill": {
     "duration": 0.024668,
     "end_time": "2024-03-25T11:56:35.239374",
     "exception": false,
     "start_time": "2024-03-25T11:56:35.214706",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ac8ec",
   "metadata": {
    "papermill": {
     "duration": 0.025094,
     "end_time": "2024-03-25T11:56:35.289611",
     "exception": false,
     "start_time": "2024-03-25T11:56:35.264517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4481457,
     "sourceId": 7681259,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4573239,
     "sourceId": 7808613,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1874.153281,
   "end_time": "2024-03-25T11:56:37.944459",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-25T11:25:23.791178",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
